{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8e3dc0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Matrix-vector product with dense matrix\n",
    "We simply gather the vector to be multiplied and then essentially perform the now local dot product.\n",
    "By default, this setup needs about 3.2 GB of memory (see `n_global`, `m_global`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cef642-7419-4ae8-804f-e6ad191ea60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed4d1db13304feab6bea0dc8794f5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 2 # number of processes\n",
    "from ipyparallel import Cluster\n",
    "cluster = await Cluster(engines=\"mpi\").start_and_connect(n=n, activate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ec14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# make a tall and skinny matrix so that we can see some speedup\n",
    "m_global = 2000000\n",
    "n_global = 200\n",
    "# TODO fix m_global % != 0\n",
    "if m_global % size != 0:\n",
    "    if rank == 0:\n",
    "        print(f\"m_global = {n_global} not divisible by size = {size}\")\n",
    "    sys.exit(1)\n",
    "if n_global % size != 0:\n",
    "    if rank == 0:\n",
    "        print(f\"n_global = {n_global} not divisible by size = {size}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "m_local = m_global // size\n",
    "n_local = n_global // size\n",
    "\n",
    "# Local rows of A: n_local x m_global\n",
    "A_local = np.random.rand(m_local, n_global)\n",
    "\n",
    "# Local part of x\n",
    "x_local = np.full(n_local, fill_value=1, dtype='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce1fb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] The 2000000x200 matmult on 2 ranks took: 0.1328679889999762 s\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "comm.Barrier()\n",
    "start = MPI.Wtime()\n",
    "# Gather full x on all ranks\n",
    "x_full = np.empty(n_global, dtype='d')\n",
    "comm.Allgather(x_local, x_full)\n",
    "\n",
    "# Local matvec\n",
    "y_local = A_local @ x_full\n",
    "comm.Barrier()\n",
    "end = MPI.Wtime()\n",
    "\n",
    "if (rank == 0): print(f\"The {m_global}x{n_global} matmult on {size} ranks took: {end - start} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc203ab-4efe-4672-b9e7-9f0db4aaf4b8",
   "metadata": {},
   "source": [
    "results:\\\n",
    "The 2000000x200 matmult on 1 ranks took: 0.21254006699999994 s\\\n",
    "The 2000000x200 matmult on 2 ranks took: 0.12854753000000008 s\\\n",
    "The 2000000x200 matmult on 4 ranks took: 0.10068987100000015 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fbcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO If we imagine a block-diagonal partitioning of the matrix, e.g.,\n",
    "#            1  2  0  |  0  3  0  |  0  4\n",
    "#    Proc0   0  5  6  |  7  0  0  |  8  0\n",
    "#            9  0 10  | 11  0  0  | 12  0\n",
    "#    -------------------------------------\n",
    "#           13  0 14  | 15 16 17  |  0  0\n",
    "#    Proc1   0 18  0  | 19 20 21  |  0  0\n",
    "#            0  0  0  | 22 23  0  | 24  0\n",
    "#    -------------------------------------\n",
    "#    Proc2  25 26 27  |  0  0 28  | 29  0\n",
    "#           30  0  0  | 31 32 33  |  0 34\n",
    "# we can write such a block matrix as [A B C\n",
    "#                                      D E F;\n",
    "#                                      H I G]\n",
    "# Then we can multiply the block diagonal parts of the matrix (A, E, and G) with our local vector without any communication.\n",
    "# Therefore, we can overlap the communication of the vector x with the local part of the matrix multiplication.\n",
    "# Use the immediate return `MPI.Iallgatherv()` to obtain the vector x from other ranks.\n",
    "# Compute the local matrix-vector product, e.g., E*x_local.\n",
    "# `MPI.Wait()` for the Iallgatherv to complete.\n",
    "# Compute the matrix-vector products with the non-block diagonal parts of the matrix and correctly sum up the results to y_local."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
